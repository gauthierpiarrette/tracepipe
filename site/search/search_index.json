{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"TracePipe","text":"<p>Row-level data lineage tracking for pandas pipelines.</p> <p>TracePipe automatically tracks what happens to every row and cell in your DataFrame \u2014 drops, transformations, merges, and value changes. Zero code changes required.</p> <ul> <li> <p> Quick Start</p> <p>Get up and running in 5 minutes</p> </li> <li> <p> User Guide</p> <p>Learn the core concepts and features</p> </li> <li> <p> API Reference</p> <p>Complete function documentation</p> </li> <li> <p> Examples</p> <p>Real-world usage patterns</p> </li> </ul>"},{"location":"#the-problem","title":"The Problem","text":"<p>Data pipelines are black boxes. When something goes wrong, you're left asking:</p> <ul> <li>\"Where did row X go?\" \u2014 Dropped somewhere, but which step?</li> <li>\"Why is this value wrong?\" \u2014 It was fine in the source, what changed it?</li> <li>\"How did these rows get merged?\" \u2014 Which parent records combined?</li> <li>\"Why are there nulls here?\" \u2014 When did they appear?</li> </ul> <pre><code>df = pd.read_csv(\"customers.csv\")\ndf = df.dropna()                          # Some rows disappear\ndf = df.merge(regions, on=\"zip\")          # New rows appear, some vanish\ndf[\"income\"] = df[\"income\"].fillna(0)     # Values change silently\ndf = df[df[\"age\"] &gt;= 18]                  # More rows gone\n# What actually happened to customer C-789?\n</code></pre> <p>Traditional debugging means <code>print()</code> statements, manual diffs, and guesswork.</p>"},{"location":"#the-solution","title":"The Solution","text":"<pre><code>import tracepipe as tp\nimport pandas as pd\n\ntp.enable(mode=\"debug\", watch=[\"income\"])\n\ndf = pd.read_csv(\"customers.csv\")\ndf = df.dropna()\ndf[\"income\"] = df[\"income\"].fillna(0)\ndf = df.merge(regions, on=\"zip\")\ndf = df[df[\"age\"] &gt;= 18]\n\n# What actually happened to customer C-789?\nprint(tp.trace(df, where={\"customer_id\": \"C-789\"}))\n</code></pre> <pre><code>Row 789 Journey:\n  Status: [DROPPED]\n  Dropped by: DataFrame.__getitem__[mask] (step 5)\n\n  Events:\n    [SURVIVED] DataFrame.dropna\n    [MODIFIED] DataFrame.fillna: income (None \u2192 0)\n    [SURVIVED] DataFrame.merge\n    [DROPPED]  DataFrame.__getitem__[mask]  \u2190 age filter\n</code></pre> <p>Now you know: C-789 had null income (filled to 0), survived the merge, but was dropped by the age filter.</p> <pre><code># Pipeline health overview\nprint(tp.check(df))\n</code></pre> <pre><code>TracePipe Check: [OK] Pipeline healthy\n  Mode: debug\n\nRetention: 847/1000 (84.7%)\nDropped: 153 rows\n  \u2022 DataFrame.dropna: 42\n  \u2022 DataFrame.__getitem__[mask]: 111\n\nValue changes: 23 cells modified\n  \u2022 DataFrame.fillna: 23 (income)\n</code></pre> <p>One import. Complete audit trail.</p>"},{"location":"#key-features","title":"Key Features","text":"Feature Description Zero-Code Instrumentation Works with existing pandas code unchanged Row-Level Tracking Know exactly where each row went Cell Provenance See before/after values for every change Merge Parent Tracking Understand which rows combined Data Contracts Validate retention rates and uniqueness HTML Reports Generate visual pipeline audits"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install tracepipe\n</code></pre> <p>For optional features:</p> <pre><code>pip install tracepipe[arrow]   # Parquet/Arrow support\npip install tracepipe[all]     # All optional dependencies\n</code></pre>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>import tracepipe as tp\nimport pandas as pd\n\n# Enable tracking\ntp.enable(mode=\"debug\", watch=[\"price\"])\n\n# Your normal pandas code\ndf = pd.DataFrame({\n    \"product\": [\"A\", \"B\", \"C\"],\n    \"price\": [10.0, None, 30.0]\n})\ndf = df.dropna()\ndf[\"price\"] = df[\"price\"] * 1.1\n\n# Inspect what happened\nprint(tp.check(df))      # Health summary\nprint(tp.trace(df, 0))   # Row 0's journey\nprint(tp.why(df, \"price\", 0))  # Why price changed\n</code></pre>"},{"location":"#whats-tracked","title":"What's Tracked","text":"Operation Tracking Completeness <code>dropna</code>, <code>drop_duplicates</code> Dropped row IDs Full <code>query</code>, <code>df[mask]</code> Dropped row IDs Full <code>head</code>, <code>tail</code>, <code>sample</code> Dropped row IDs Full <code>fillna</code>, <code>replace</code> Cell diffs (watched cols) Full <code>loc[]=</code>, <code>iloc[]=</code>, <code>at[]=</code> Cell diffs Full <code>merge</code>, <code>join</code> Parent tracking Full <code>groupby().agg()</code> Group membership Full <code>apply</code>, <code>pipe</code> Output tracked Partial"},{"location":"#license","title":"License","text":"<p>TracePipe is released under the MIT License.</p>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to TracePipe will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#unreleased","title":"[Unreleased]","text":""},{"location":"changelog/#030-2026-02-03","title":"[0.3.0] - 2026-02-03","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>MkDocs documentation site with Material theme</li> <li>Comprehensive API reference documentation</li> <li>Getting started guides and tutorials</li> <li><code>tp.register()</code> API for manually registering DataFrames created before <code>enable()</code></li> <li>Configurable retention threshold in <code>tp.check()</code></li> <li>Ghost row capture for fallback filter paths</li> <li>Comprehensive test coverage for COLUMN identity mode</li> <li>Data quality contracts with fluent API (<code>tp.contract().expect_*()</code>)</li> <li>HTML report generation with <code>tp.report()</code></li> <li>Snapshot and diff functionality</li> <li>Debug mode with cell-level tracking</li> <li><code>tp.why()</code> for cell provenance</li> <li><code>tp.trace()</code> for row journey</li> <li>Watched columns for selective tracking</li> <li>Ghost values capture</li> <li>Basic row-level lineage tracking</li> <li>Support for filter operations (dropna, query, boolean indexing)</li> <li>Support for transform operations (fillna, replace, setitem)</li> <li>Support for merge and join operations</li> <li>CI and Debug modes</li> </ul>"},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>Recursion bug when accessing hidden <code>__tracepipe_row_id__</code> column in COLUMN mode</li> <li>Config propagation to <code>row_manager</code> and <code>store</code> components in <code>enable()</code></li> <li>Retention rate calculation for multi-table pipelines with merges</li> <li>Export wrappers (<code>to_csv</code>, <code>to_parquet</code>) now correctly strip hidden column</li> <li><code>_filter_op_depth</code> cleanup in error scenarios</li> </ul>"},{"location":"contributing/","title":"Contributing","text":"<p>Thank you for your interest in contributing to TracePipe!</p>"},{"location":"contributing/#development-setup","title":"Development Setup","text":""},{"location":"contributing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.9+</li> <li>uv (recommended) or pip</li> </ul>"},{"location":"contributing/#setup","title":"Setup","text":"<pre><code># Clone the repository\ngit clone https://github.com/tracepipe/tracepipe.git\ncd tracepipe\n\n# Install with uv (recommended)\nuv sync --all-extras\n\n# Or with pip\npip install -e \".[dev]\"\n</code></pre>"},{"location":"contributing/#verify-setup","title":"Verify Setup","text":"<pre><code># Run tests\nuv run pytest tests/ -v\n\n# Run linting\nuv run task lint\n\n# Run full checks\nuv run task check\n</code></pre>"},{"location":"contributing/#development-workflow","title":"Development Workflow","text":""},{"location":"contributing/#1-create-a-branch","title":"1. Create a Branch","text":"<pre><code>git checkout -b feature/your-feature-name\n# or\ngit checkout -b fix/your-bug-fix\n</code></pre>"},{"location":"contributing/#2-make-changes","title":"2. Make Changes","text":"<ul> <li>Write code following the existing style</li> <li>Add tests for new functionality</li> <li>Update documentation as needed</li> </ul>"},{"location":"contributing/#3-run-tests","title":"3. Run Tests","text":"<pre><code># Run all tests\nuv run pytest tests/ -v\n\n# Run specific test file\nuv run pytest tests/test_api.py -v\n\n# Run with coverage\nuv run pytest tests/ --cov=tracepipe --cov-report=term-missing\n</code></pre>"},{"location":"contributing/#4-run-linting","title":"4. Run Linting","text":"<pre><code># Check linting\nuv run task lint\n\n# Auto-fix formatting\nuv run task format\n</code></pre>"},{"location":"contributing/#5-submit-pr","title":"5. Submit PR","text":"<ul> <li>Push your branch</li> <li>Create a Pull Request</li> <li>Fill out the PR template</li> </ul>"},{"location":"contributing/#code-style","title":"Code Style","text":""},{"location":"contributing/#python-style","title":"Python Style","text":"<ul> <li>Follow PEP 8</li> <li>Use type hints for all public functions</li> <li>Maximum line length: 100 characters</li> <li>Use Black for formatting</li> <li>Use Ruff for linting</li> </ul>"},{"location":"contributing/#docstrings","title":"Docstrings","text":"<p>Use Google-style docstrings:</p> <pre><code>def example_function(param1: str, param2: int = 10) -&gt; bool:\n    \"\"\"Short description of the function.\n\n    Longer description if needed. Can span multiple lines.\n\n    Args:\n        param1: Description of param1.\n        param2: Description of param2. Defaults to 10.\n\n    Returns:\n        Description of return value.\n\n    Raises:\n        ValueError: When param1 is empty.\n\n    Example:\n        &gt;&gt;&gt; example_function(\"test\")\n        True\n    \"\"\"\n</code></pre>"},{"location":"contributing/#testing","title":"Testing","text":"<ul> <li>Place tests in <code>tests/</code> directory</li> <li>Name test files <code>test_*.py</code></li> <li>Name test functions <code>test_*</code></li> <li>Use pytest fixtures for setup</li> <li>Aim for high coverage on new code</li> </ul>"},{"location":"contributing/#project-structure","title":"Project Structure","text":"<pre><code>tracepipe/\n\u251c\u2500\u2500 tracepipe/              # Main package\n\u2502   \u251c\u2500\u2500 __init__.py         # Public API exports\n\u2502   \u251c\u2500\u2500 api.py              # Core API functions\n\u2502   \u251c\u2500\u2500 contracts.py        # Contract builder\n\u2502   \u251c\u2500\u2500 convenience.py      # check/trace/why functions\n\u2502   \u251c\u2500\u2500 core.py             # Configuration and context\n\u2502   \u251c\u2500\u2500 instrumentation/    # Pandas monkey-patching\n\u2502   \u2502   \u251c\u2500\u2500 pandas_inst.py  # Main instrumentation\n\u2502   \u2502   \u251c\u2500\u2500 filter_capture.py\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 storage/            # Lineage storage\n\u2502       \u251c\u2500\u2500 lineage_store.py\n\u2502       \u2514\u2500\u2500 row_identity.py\n\u251c\u2500\u2500 tests/                  # Test suite\n\u251c\u2500\u2500 examples/               # Example scripts\n\u251c\u2500\u2500 docs/                   # Documentation\n\u2514\u2500\u2500 benchmarks/             # Performance benchmarks\n</code></pre>"},{"location":"contributing/#adding-features","title":"Adding Features","text":""},{"location":"contributing/#1-new-pandas-operation-support","title":"1. New Pandas Operation Support","text":"<p>To add tracking for a new pandas operation:</p> <ol> <li>Identify the operation type (filter, transform, merge, etc.)</li> <li>Add wrapper in appropriate <code>tracepipe/instrumentation/</code> module</li> <li>Register in <code>pandas_inst.py</code></li> <li>Add tests in <code>tests/</code></li> <li>Document in README and docs</li> </ol>"},{"location":"contributing/#2-new-contract-expectation","title":"2. New Contract Expectation","text":"<p>To add a new contract expectation:</p> <ol> <li>Add method to <code>ContractBuilder</code> in <code>contracts.py</code></li> <li>Implement validation logic</li> <li>Add tests in <code>tests/test_contracts.py</code></li> <li>Document in <code>docs/guide/contracts.md</code></li> </ol>"},{"location":"contributing/#running-benchmarks","title":"Running Benchmarks","text":"<pre><code>cd benchmarks\npython run_all.py\n</code></pre>"},{"location":"contributing/#documentation","title":"Documentation","text":""},{"location":"contributing/#local-preview","title":"Local Preview","text":"<pre><code># Install mkdocs\npip install mkdocs mkdocs-material mkdocstrings[python]\n\n# Serve locally\nmkdocs serve\n</code></pre>"},{"location":"contributing/#building-docs","title":"Building Docs","text":"<pre><code>mkdocs build\n</code></pre>"},{"location":"contributing/#release-process","title":"Release Process","text":"<p>Releases are managed by maintainers:</p> <ol> <li>Update version in <code>pyproject.toml</code></li> <li>Update CHANGELOG.md</li> <li>Create git tag</li> <li>GitHub Actions builds and publishes to PyPI</li> </ol>"},{"location":"contributing/#getting-help","title":"Getting Help","text":"<ul> <li>Open an issue for bugs or feature requests</li> <li>Start a discussion for questions</li> <li>Tag maintainers for urgent issues</li> </ul>"},{"location":"contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>Please be respectful and inclusive. We follow the Contributor Covenant.</p>"},{"location":"api/","title":"API Reference","text":"<p>TracePipe provides a minimal API surface for maximum functionality.</p>"},{"location":"api/#quick-reference","title":"Quick Reference","text":""},{"location":"api/#core-functions-6","title":"Core Functions (6)","text":"Function Purpose <code>tp.enable()</code> Start tracking <code>tp.disable()</code> Stop tracking <code>tp.reset()</code> Clear lineage data <code>tp.check()</code> Health audit <code>tp.trace()</code> Row journey <code>tp.why()</code> Cell provenance"},{"location":"api/#additional-functions","title":"Additional Functions","text":"Function Purpose <code>tp.stage()</code> Label pipeline stages <code>tp.register()</code> Manually register DataFrames <code>tp.report()</code> Generate HTML report <code>tp.snapshot()</code> Capture DataFrame state <code>tp.diff()</code> Compare snapshots"},{"location":"api/#namespaces","title":"Namespaces","text":"Namespace Purpose <code>tp.contracts</code> Data quality contracts <code>tp.debug</code> Advanced debugging tools"},{"location":"api/#import","title":"Import","text":"<pre><code>import tracepipe as tp\n</code></pre> <p>All public functions are available directly from the <code>tp</code> namespace.</p>"},{"location":"api/#result-objects","title":"Result Objects","text":"<p>All query functions return structured result objects:</p> Function Returns <code>tp.check()</code> <code>CheckResult</code> <code>tp.trace()</code> <code>TraceResult</code> <code>tp.why()</code> <code>WhyResult</code> <code>tp.contract().check()</code> <code>ContractResult</code> <code>tp.snapshot()</code> <code>Snapshot</code> <code>tp.diff()</code> <code>DiffResult</code> <p>All result objects support:</p> <ul> <li><code>print(result)</code> \u2014 Pretty-printed output</li> <li><code>result.to_dict()</code> \u2014 Dictionary representation</li> <li>Boolean evaluation \u2014 <code>if result.passed:</code></li> </ul>"},{"location":"api/#type-hints","title":"Type Hints","text":"<p>TracePipe is fully typed. IDE autocompletion works out of the box:</p> <pre><code>import tracepipe as tp\n\nresult = tp.check(df)  # Autocomplete shows CheckResult methods\nresult.retention       # Type: float\nresult.warnings        # Type: list[str]\n</code></pre>"},{"location":"api/contracts/","title":"Contracts API","text":"<p>Data quality contracts for validation.</p>"},{"location":"api/contracts/#creating-contracts","title":"Creating Contracts","text":""},{"location":"api/contracts/#contract","title":"contract","text":"<pre><code>tp.contract() -&gt; ContractBuilder\n</code></pre> <p>Create a new contract builder.</p> <p>Returns: <code>ContractBuilder</code> with chainable methods.</p> <p>Example:</p> <pre><code>result = (tp.contract()\n    .expect_unique(\"id\")\n    .expect_no_nulls(\"email\")\n    .check(df))\n</code></pre>"},{"location":"api/contracts/#contract-methods","title":"Contract Methods","text":""},{"location":"api/contracts/#expect_unique","title":"expect_unique","text":"<pre><code>.expect_unique(column: str) -&gt; ContractBuilder\n</code></pre> <p>Expect no duplicate values in column.</p> <p>Example:</p> <pre><code>.expect_unique(\"order_id\")\n</code></pre>"},{"location":"api/contracts/#expect_no_nulls","title":"expect_no_nulls","text":"<pre><code>.expect_no_nulls(columns: str | list[str]) -&gt; ContractBuilder\n</code></pre> <p>Expect no null values.</p> <p>Example:</p> <pre><code>.expect_no_nulls(\"customer_id\")\n.expect_no_nulls([\"name\", \"email\"])\n</code></pre>"},{"location":"api/contracts/#expect_retention","title":"expect_retention","text":"<pre><code>.expect_retention(min_rate: float) -&gt; ContractBuilder\n</code></pre> <p>Expect minimum row retention rate.</p> <p>Parameters:</p> Parameter Type Description <code>min_rate</code> <code>float</code> Minimum retention (0-1) <p>Example:</p> <pre><code>.expect_retention(min_rate=0.9)  # At least 90%\n</code></pre>"},{"location":"api/contracts/#expect_no_drops","title":"expect_no_drops","text":"<pre><code>.expect_no_drops() -&gt; ContractBuilder\n</code></pre> <p>Expect no rows were dropped.</p> <p>Example:</p> <pre><code>.expect_no_drops()\n</code></pre>"},{"location":"api/contracts/#expect_columns","title":"expect_columns","text":"<pre><code>.expect_columns(columns: list[str]) -&gt; ContractBuilder\n</code></pre> <p>Expect columns to exist.</p> <p>Example:</p> <pre><code>.expect_columns([\"id\", \"name\", \"email\"])\n</code></pre>"},{"location":"api/contracts/#expect_dtypes","title":"expect_dtypes","text":"<pre><code>.expect_dtypes(dtypes: dict[str, str]) -&gt; ContractBuilder\n</code></pre> <p>Expect column data types.</p> <p>Example:</p> <pre><code>.expect_dtypes({\n    \"id\": \"int64\",\n    \"price\": \"float64\",\n})\n</code></pre>"},{"location":"api/contracts/#expect_range","title":"expect_range","text":"<pre><code>.expect_range(\n    column: str,\n    min_val: float | None = None,\n    max_val: float | None = None,\n) -&gt; ContractBuilder\n</code></pre> <p>Expect values within range.</p> <p>Example:</p> <pre><code>.expect_range(\"age\", min_val=0, max_val=150)\n.expect_range(\"price\", min_val=0)  # No max\n</code></pre>"},{"location":"api/contracts/#expect_values","title":"expect_values","text":"<pre><code>.expect_values(column: str, allowed: list[Any]) -&gt; ContractBuilder\n</code></pre> <p>Expect values from allowed set.</p> <p>Example:</p> <pre><code>.expect_values(\"status\", [\"active\", \"inactive\"])\n</code></pre>"},{"location":"api/contracts/#check","title":"check","text":"<pre><code>.check(df: pd.DataFrame) -&gt; ContractResult\n</code></pre> <p>Execute the contract against a DataFrame.</p> <p>Returns: <code>ContractResult</code></p> Attribute Type Description <code>.passed</code> <code>bool</code> All expectations met <code>.expectations</code> <code>list</code> All expectations <code>.failures</code> <code>list</code> Failed expectations"},{"location":"api/contracts/#contractresult-methods","title":"ContractResult Methods","text":""},{"location":"api/contracts/#raise_if_failed","title":"raise_if_failed","text":"<pre><code>result.raise_if_failed() -&gt; None\n</code></pre> <p>Raise <code>ContractViolationError</code> if contract failed.</p> <p>Example:</p> <pre><code>result = tp.contract().expect_unique(\"id\").check(df)\nresult.raise_if_failed()  # Raises if duplicates found\n</code></pre>"},{"location":"api/contracts/#complete-example","title":"Complete Example","text":"<pre><code>import tracepipe as tp\n\ntp.enable(mode=\"ci\")\n\ndf = process_pipeline(raw_data)\n\n# Define and check contract\nresult = (tp.contract()\n    # Schema\n    .expect_columns([\"id\", \"email\", \"amount\"])\n    .expect_dtypes({\"amount\": \"float64\"})\n\n    # Quality\n    .expect_unique(\"id\")\n    .expect_no_nulls([\"id\", \"email\"])\n    .expect_range(\"amount\", min_val=0)\n    .expect_values(\"status\", [\"pending\", \"complete\", \"failed\"])\n\n    # Pipeline health\n    .expect_retention(min_rate=0.8)\n\n    .check(df))\n\n# Handle result\nif result.passed:\n    print(\"\u2713 All contracts passed\")\nelse:\n    print(\"\u2717 Contract violations:\")\n    for failure in result.failures:\n        print(f\"  - {failure}\")\n    result.raise_if_failed()\n</code></pre>"},{"location":"api/core/","title":"Core API","text":""},{"location":"api/core/#tracking-control","title":"Tracking Control","text":""},{"location":"api/core/#enable","title":"enable","text":"<pre><code>tp.enable(\n    mode: str = \"ci\",\n    watch: list[str] | None = None,\n    backend: str | None = None,\n    identity: str | None = None,\n) -&gt; None\n</code></pre> <p>Start TracePipe tracking.</p> <p>Parameters:</p> Parameter Type Default Description <code>mode</code> <code>str</code> <code>\"ci\"</code> Tracking mode: <code>\"ci\"</code> or <code>\"debug\"</code> <code>watch</code> <code>list[str]</code> <code>None</code> Columns to track for cell changes (debug mode) <code>backend</code> <code>str</code> <code>None</code> Lineage storage backend <code>identity</code> <code>str</code> <code>None</code> Row identity strategy <p>Example:</p> <pre><code># CI mode - lightweight\ntp.enable(mode=\"ci\")\n\n# Debug mode with watched columns\ntp.enable(mode=\"debug\", watch=[\"price\", \"quantity\", \"status\"])\n</code></pre>"},{"location":"api/core/#disable","title":"disable","text":"<pre><code>tp.disable() -&gt; None\n</code></pre> <p>Stop TracePipe tracking. Restores original pandas methods.</p> <p>Example:</p> <pre><code>tp.enable()\n# ... tracked operations ...\ntp.disable()  # Back to normal pandas\n</code></pre>"},{"location":"api/core/#reset","title":"reset","text":"<pre><code>tp.reset() -&gt; None\n</code></pre> <p>Clear all lineage data. Does not disable tracking.</p> <p>Example:</p> <pre><code>tp.enable()\ndf = process_data_v1()\ntp.check(df)\n\ntp.reset()  # Clear lineage, keep tracking enabled\ndf = process_data_v2()\ntp.check(df)\n</code></pre>"},{"location":"api/core/#register","title":"register","text":"<pre><code>tp.register(*dfs: pd.DataFrame) -&gt; None\n</code></pre> <p>Manually register DataFrames for tracking.</p> <p>Use this when DataFrames are created before <code>tp.enable()</code> is called.</p> <p>Parameters:</p> Parameter Type Description <code>*dfs</code> <code>pd.DataFrame</code> One or more DataFrames to register <p>Example:</p> <pre><code># DataFrames created before enable()\ncustomers = pd.read_csv(\"customers.csv\")\norders = pd.read_csv(\"orders.csv\")\n\ntp.enable(mode=\"debug\")\ntp.register(customers, orders)  # Now they're tracked\n</code></pre>"},{"location":"api/core/#stage","title":"stage","text":"<pre><code>tp.stage(name: str) -&gt; None\n</code></pre> <p>Label the current pipeline stage.</p> <p>Parameters:</p> Parameter Type Description <code>name</code> <code>str</code> Stage name <p>Example:</p> <pre><code>tp.stage(\"load\")\ndf = pd.read_csv(\"data.csv\")\n\ntp.stage(\"clean\")\ndf = df.dropna()\n\ntp.stage(\"transform\")\ndf[\"total\"] = df[\"price\"] * df[\"qty\"]\n</code></pre>"},{"location":"api/core/#query-functions","title":"Query Functions","text":""},{"location":"api/core/#check","title":"check","text":"<pre><code>tp.check(\n    df: pd.DataFrame,\n    *,\n    retention_threshold: float | None = None,\n    merge_expansion_threshold: float | None = None,\n) -&gt; CheckResult\n</code></pre> <p>Health check for a DataFrame's lineage.</p> <p>Parameters:</p> Parameter Type Default Description <code>df</code> <code>pd.DataFrame</code> required DataFrame to check <code>retention_threshold</code> <code>float</code> <code>0.5</code> Warn if retention below this <code>merge_expansion_threshold</code> <code>float</code> <code>None</code> Warn if merge expands beyond this <p>Returns: <code>CheckResult</code></p> Attribute Type Description <code>.passed</code> <code>bool</code> True if healthy <code>.mode</code> <code>str</code> Current tracking mode <code>.retention</code> <code>float</code> Row retention rate (0-1) <code>.n_dropped</code> <code>int</code> Total dropped rows <code>.n_changes</code> <code>int</code> Total cell changes <code>.warnings</code> <code>list[str]</code> Any warnings <code>.drops_by_op</code> <code>dict</code> Drops by operation <code>.changes_by_op</code> <code>dict</code> Changes by operation <p>Example:</p> <pre><code>result = tp.check(df)\nprint(result)\n\nif not result.passed:\n    for warning in result.warnings:\n        print(f\"\u26a0 {warning}\")\n</code></pre>"},{"location":"api/core/#trace","title":"trace","text":"<pre><code>tp.trace(\n    df: pd.DataFrame,\n    row: int | None = None,\n    *,\n    where: dict[str, Any] | None = None,\n) -&gt; TraceResult\n</code></pre> <p>Trace a row's journey through the pipeline.</p> <p>Parameters:</p> Parameter Type Default Description <code>df</code> <code>pd.DataFrame</code> required DataFrame containing the row <code>row</code> <code>int</code> <code>None</code> Row index (0-based) <code>where</code> <code>dict</code> <code>None</code> Business key lookup <p>Returns: <code>TraceResult</code></p> Attribute Type Description <code>.row_id</code> <code>int</code> Internal row ID <code>.status</code> <code>str</code> <code>\"alive\"</code> or <code>\"dropped\"</code> <code>.events</code> <code>list</code> All events for this row <code>.dropped_by</code> <code>str</code> Operation that dropped (if dropped) <p>Example:</p> <pre><code># By index\ntrace = tp.trace(df, row=0)\n\n# By business key\ntrace = tp.trace(df, where={\"customer_id\": \"C-123\"})\n\nprint(trace)\n</code></pre>"},{"location":"api/core/#why","title":"why","text":"<pre><code>tp.why(\n    df: pd.DataFrame,\n    col: str,\n    row: int | None = None,\n    *,\n    where: dict[str, Any] | None = None,\n) -&gt; WhyResult\n</code></pre> <p>Explain why a cell has its current value.</p> <p>Parameters:</p> Parameter Type Default Description <code>df</code> <code>pd.DataFrame</code> required DataFrame containing the cell <code>col</code> <code>str</code> required Column name <code>row</code> <code>int</code> <code>None</code> Row index (0-based) <code>where</code> <code>dict</code> <code>None</code> Business key lookup <p>Returns: <code>WhyResult</code></p> Attribute Type Description <code>.column</code> <code>str</code> Column name <code>.row_id</code> <code>int</code> Internal row ID <code>.current_value</code> <code>Any</code> Current cell value <code>.history</code> <code>list</code> All changes to this cell <code>.was_null</code> <code>bool</code> Was ever null <code>.null_recovered</code> <code>bool</code> Null was later filled <p>Example:</p> <pre><code>why = tp.why(df, col=\"income\", row=0)\nprint(why)\n\nfor change in why.history:\n    print(f\"{change.old_value} \u2192 {change.new_value}\")\n</code></pre> <p>Requires Debug Mode</p> <p><code>tp.why()</code> requires debug mode with the column being watched.</p>"},{"location":"api/core/#output-functions","title":"Output Functions","text":""},{"location":"api/core/#report","title":"report","text":"<pre><code>tp.report(\n    df: pd.DataFrame,\n    path: str,\n    *,\n    title: str | None = None,\n    include_data: bool = False,\n) -&gt; None\n</code></pre> <p>Generate an HTML report.</p> <p>Parameters:</p> Parameter Type Default Description <code>df</code> <code>pd.DataFrame</code> required DataFrame to report on <code>path</code> <code>str</code> required Output file path <code>title</code> <code>str</code> <code>None</code> Report title <code>include_data</code> <code>bool</code> <code>False</code> Include data preview <p>Example:</p> <pre><code>tp.report(df, \"audit.html\", title=\"Pipeline Audit - 2024-01\")\n</code></pre>"},{"location":"api/core/#snapshot","title":"snapshot","text":"<pre><code>tp.snapshot(\n    df: pd.DataFrame,\n    *,\n    include_data: bool = False,\n    path: str | None = None,\n) -&gt; Snapshot\n</code></pre> <p>Capture DataFrame state for later comparison.</p> <p>Parameters:</p> Parameter Type Default Description <code>df</code> <code>pd.DataFrame</code> required DataFrame to snapshot <code>include_data</code> <code>bool</code> <code>False</code> Store full data copy <code>path</code> <code>str</code> <code>None</code> Save to disk <p>Returns: <code>Snapshot</code></p> <p>Example:</p> <pre><code>before = tp.snapshot(df)\ndf = df.dropna()\nafter = tp.snapshot(df)\n</code></pre>"},{"location":"api/core/#diff","title":"diff","text":"<pre><code>tp.diff(\n    before: Snapshot,\n    after: Snapshot,\n) -&gt; DiffResult\n</code></pre> <p>Compare two snapshots.</p> <p>Parameters:</p> Parameter Type Description <code>before</code> <code>Snapshot</code> Earlier snapshot <code>after</code> <code>Snapshot</code> Later snapshot <p>Returns: <code>DiffResult</code></p> Attribute Type Description <code>.rows_added</code> <code>int</code> New rows <code>.rows_removed</code> <code>int</code> Removed rows <code>.cells_changed</code> <code>int</code> Modified cells <p>Example:</p> <pre><code>diff = tp.diff(before, after)\nprint(f\"Removed: {diff.rows_removed} rows\")\n</code></pre>"},{"location":"api/debug/","title":"Debug API","text":"<p>Advanced debugging and inspection tools.</p>"},{"location":"api/debug/#accessing-debug-tools","title":"Accessing Debug Tools","text":""},{"location":"api/debug/#inspect","title":"inspect","text":"<pre><code>tp.debug.inspect() -&gt; DebugInspector\n</code></pre> <p>Get a debug inspector for raw lineage access.</p> <p>Returns: <code>DebugInspector</code></p> <p>Example:</p> <pre><code>dbg = tp.debug.inspect()\nprint(f\"Steps recorded: {len(dbg.steps)}\")\n</code></pre>"},{"location":"api/debug/#debuginspector-properties","title":"DebugInspector Properties","text":""},{"location":"api/debug/#steps","title":"steps","text":"<pre><code>dbg.steps -&gt; list[Step]\n</code></pre> <p>All recorded pipeline steps.</p> <p>Each <code>Step</code> contains:</p> Attribute Type Description <code>.operation</code> <code>str</code> Operation name <code>.input_shape</code> <code>tuple</code> Input (rows, cols) <code>.output_shape</code> <code>tuple</code> Output (rows, cols) <code>.timestamp</code> <code>datetime</code> When it occurred <code>.stage</code> <code>str</code> Pipeline stage (if set) <p>Example:</p> <pre><code>for step in dbg.steps:\n    print(f\"{step.operation}: {step.input_shape} \u2192 {step.output_shape}\")\n</code></pre>"},{"location":"api/debug/#debuginspector-methods","title":"DebugInspector Methods","text":""},{"location":"api/debug/#dropped_rows","title":"dropped_rows","text":"<pre><code>dbg.dropped_rows() -&gt; set[int]\n</code></pre> <p>Get IDs of all dropped rows.</p> <p>Example:</p> <pre><code>dropped = dbg.dropped_rows()\nprint(f\"Total dropped: {len(dropped)}\")\n</code></pre>"},{"location":"api/debug/#explain_row","title":"explain_row","text":"<pre><code>dbg.explain_row(row_id: int) -&gt; RowExplanation\n</code></pre> <p>Get detailed explanation for a specific row.</p> <p>Parameters:</p> Parameter Type Description <code>row_id</code> <code>int</code> Internal row ID <p>Returns: <code>RowExplanation</code></p> <p>Example:</p> <pre><code>for rid in list(dbg.dropped_rows())[:5]:\n    explanation = dbg.explain_row(rid)\n    print(f\"Row {rid}: {explanation.status}\")\n</code></pre>"},{"location":"api/debug/#explain_group","title":"explain_group","text":"<pre><code>dbg.explain_group(group_key: Any) -&gt; GroupExplanation\n</code></pre> <p>Explain which rows belonged to a group (after groupby).</p> <p>Example:</p> <pre><code># After: df.groupby(\"category\").sum()\nexplanation = dbg.explain_group(\"Electronics\")\nprint(f\"Group 'Electronics' had {len(explanation.member_ids)} rows\")\n</code></pre>"},{"location":"api/debug/#get_ghost_values","title":"get_ghost_values","text":"<pre><code>dbg.get_ghost_values(row_id: int) -&gt; dict[str, Any] | None\n</code></pre> <p>Get last known values of a dropped row.</p> <p>Example:</p> <pre><code>dropped_rid = list(dbg.dropped_rows())[0]\nghost = dbg.get_ghost_values(dropped_rid)\nif ghost:\n    print(f\"Last values: {ghost}\")\n</code></pre>"},{"location":"api/debug/#stats","title":"stats","text":"<pre><code>dbg.stats() -&gt; dict\n</code></pre> <p>Get tracking statistics.</p> <p>Returns:</p> <pre><code>{\n    \"steps_recorded\": 15,\n    \"rows_tracked\": 1000,\n    \"rows_dropped\": 153,\n    \"cells_tracked\": 5000,\n    \"memory_bytes\": 102400,  # If psutil available\n}\n</code></pre>"},{"location":"api/debug/#export","title":"export","text":"<pre><code>dbg.export(format: str, path: str | None = None) -&gt; dict | None\n</code></pre> <p>Export lineage data.</p> <p>Parameters:</p> Parameter Type Description <code>format</code> <code>str</code> <code>\"json\"</code>, <code>\"dict\"</code>, or <code>\"csv\"</code> <code>path</code> <code>str</code> Output file (optional) <p>Returns: Data dict if <code>path</code> is None, else None.</p> <p>Example:</p> <pre><code># Export to file\ndbg.export(\"json\", \"lineage.json\")\n\n# Get as dict\ndata = dbg.export(\"dict\")\n</code></pre>"},{"location":"api/debug/#complete-example","title":"Complete Example","text":"<pre><code>import tracepipe as tp\n\ntp.enable(mode=\"debug\", watch=[\"price\", \"status\"])\n\n# Run pipeline\ndf = pd.read_csv(\"data.csv\")\ndf = df.dropna()\ndf[\"price\"] = df[\"price\"] * 1.1\ndf = df[df[\"price\"] &gt; 10]\n\n# Deep inspection\ndbg = tp.debug.inspect()\n\n# Review all steps\nprint(\"Pipeline steps:\")\nfor i, step in enumerate(dbg.steps):\n    print(f\"  {i+1}. {step.operation}\")\n    print(f\"     {step.input_shape} \u2192 {step.output_shape}\")\n\n# Investigate dropped rows\ndropped = dbg.dropped_rows()\nprint(f\"\\nDropped {len(dropped)} rows\")\n\n# Look at specific dropped rows\nfor rid in list(dropped)[:3]:\n    ghost = dbg.get_ghost_values(rid)\n    if ghost:\n        print(f\"  Row {rid}: price was {ghost.get('price')}\")\n\n# Export for external analysis\ndbg.export(\"json\", \"pipeline_lineage.json\")\n\n# Stats\nstats = dbg.stats()\nprint(f\"\\nMemory used: {stats.get('memory_bytes', 'N/A')} bytes\")\n</code></pre>"},{"location":"examples/data-validation/","title":"Data Validation Example","text":"<p>Using TracePipe contracts for data quality validation in CI/CD.</p>"},{"location":"examples/data-validation/#the-scenario","title":"The Scenario","text":"<p>You have an ETL pipeline that runs daily. You need to:</p> <ol> <li>Validate incoming data meets expectations</li> <li>Ensure transformations don't drop too many rows</li> <li>Gate deployments on data quality</li> <li>Alert on anomalies</li> </ol>"},{"location":"examples/data-validation/#cicd-integration","title":"CI/CD Integration","text":""},{"location":"examples/data-validation/#basic-validation-script","title":"Basic Validation Script","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"validate_pipeline.py - Run as part of CI/CD\"\"\"\n\nimport sys\nimport tracepipe as tp\nimport pandas as pd\n\ndef main():\n    tp.enable(mode=\"ci\")  # Lightweight for CI\n\n    # Load and process data\n    df = pd.read_csv(\"data/daily_extract.csv\")\n    df = df.dropna(subset=[\"id\", \"amount\"])\n    df = df[df[\"amount\"] &gt; 0]\n    df = df.drop_duplicates(subset=[\"id\"])\n\n    # Define contract\n    result = (tp.contract()\n        # Schema validation\n        .expect_columns([\"id\", \"amount\", \"timestamp\", \"category\"])\n        .expect_dtypes({\n            \"id\": \"object\",\n            \"amount\": \"float64\",\n        })\n\n        # Data quality\n        .expect_unique(\"id\")\n        .expect_no_nulls([\"id\", \"amount\"])\n        .expect_range(\"amount\", min_val=0)\n        .expect_values(\"category\", [\"A\", \"B\", \"C\", \"D\"])\n\n        # Pipeline health\n        .expect_retention(min_rate=0.9)  # At least 90% retained\n\n        .check(df))\n\n    # Report results\n    print(result)\n\n    if not result.passed:\n        print(\"\\n\u274c Data validation FAILED\")\n        print(\"\\nFailures:\")\n        for failure in result.failures:\n            print(f\"  \u2022 {failure}\")\n        sys.exit(1)\n\n    print(\"\\n\u2705 Data validation PASSED\")\n    tp.disable()\n    return 0\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n</code></pre>"},{"location":"examples/data-validation/#github-actions-workflow","title":"GitHub Actions Workflow","text":"<pre><code># .github/workflows/data-validation.yml\nname: Data Validation\n\non:\n  schedule:\n    - cron: '0 6 * * *'  # Daily at 6 AM\n  workflow_dispatch:\n\njobs:\n  validate:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: '3.11'\n\n      - name: Install dependencies\n        run: pip install tracepipe pandas\n\n      - name: Download data\n        run: ./scripts/download_daily_data.sh\n\n      - name: Validate pipeline\n        run: python validate_pipeline.py\n\n      - name: Upload lineage report\n        if: always()\n        uses: actions/upload-artifact@v4\n        with:\n          name: lineage-report\n          path: pipeline_audit.html\n</code></pre>"},{"location":"examples/data-validation/#multi-stage-validation","title":"Multi-Stage Validation","text":""},{"location":"examples/data-validation/#checkpoint-validation","title":"Checkpoint Validation","text":"<pre><code>import tracepipe as tp\nimport pandas as pd\n\ndef validate_checkpoint(df, checkpoint_name, contract):\n    \"\"\"Validate a pipeline checkpoint\"\"\"\n    result = contract.check(df)\n\n    if not result.passed:\n        raise ValueError(f\"Checkpoint '{checkpoint_name}' failed: {result.failures}\")\n\n    print(f\"\u2713 Checkpoint '{checkpoint_name}' passed\")\n    return df\n\ntp.enable(mode=\"ci\")\n\n# Stage 1: Raw data\ndf = pd.read_csv(\"raw_data.csv\")\nvalidate_checkpoint(df, \"raw_load\", \n    tp.contract()\n        .expect_columns([\"id\", \"value\", \"timestamp\"]))\n\n# Stage 2: Cleaned data\ntp.stage(\"clean\")\ndf = df.dropna()\ndf = df[df[\"value\"] &gt; 0]\nvalidate_checkpoint(df, \"cleaned\",\n    tp.contract()\n        .expect_no_nulls([\"id\", \"value\"])\n        .expect_retention(min_rate=0.95))\n\n# Stage 3: Enriched data\ntp.stage(\"enrich\")\ndf = df.merge(lookup_table, on=\"id\", how=\"left\")\nvalidate_checkpoint(df, \"enriched\",\n    tp.contract()\n        .expect_columns([\"id\", \"value\", \"category\", \"region\"]))\n\n# Final validation\nresult = tp.contract().expect_retention(min_rate=0.9).check(df)\nresult.raise_if_failed()\n\ntp.disable()\n</code></pre>"},{"location":"examples/data-validation/#alerting-on-anomalies","title":"Alerting on Anomalies","text":""},{"location":"examples/data-validation/#slack-integration","title":"Slack Integration","text":"<pre><code>import tracepipe as tp\nimport pandas as pd\nimport requests\n\nSLACK_WEBHOOK = \"https://hooks.slack.com/services/...\"\n\ndef send_slack_alert(message):\n    requests.post(SLACK_WEBHOOK, json={\"text\": message})\n\ndef run_pipeline_with_alerting():\n    tp.enable(mode=\"ci\")\n\n    df = pd.read_csv(\"data.csv\")\n    df = df.dropna()\n    df = df[df[\"status\"] == \"active\"]\n\n    result = tp.check(df)\n\n    # Alert on low retention\n    if result.retention &lt; 0.8:\n        send_slack_alert(\n            f\"\u26a0\ufe0f Pipeline Alert: Low retention ({result.retention:.1%})\\n\"\n            f\"Dropped {result.n_dropped} rows\"\n        )\n\n    # Alert on contract failures\n    contract_result = (tp.contract()\n        .expect_retention(min_rate=0.9)\n        .expect_unique(\"id\")\n        .check(df))\n\n    if not contract_result.passed:\n        failures = \"\\n\".join(f\"\u2022 {f}\" for f in contract_result.failures)\n        send_slack_alert(\n            f\"\ud83d\udea8 Data Contract Violation!\\n{failures}\"\n        )\n\n    tp.disable()\n    return df\n</code></pre>"},{"location":"examples/data-validation/#regression-testing","title":"Regression Testing","text":""},{"location":"examples/data-validation/#baseline-comparison","title":"Baseline Comparison","text":"<pre><code>import tracepipe as tp\nimport pandas as pd\nimport json\n\ndef save_baseline(df, path=\"baseline_stats.json\"):\n    \"\"\"Save current stats as baseline\"\"\"\n    result = tp.check(df)\n    baseline = {\n        \"retention\": result.retention,\n        \"n_rows\": len(df),\n        \"n_dropped\": result.n_dropped,\n        \"drops_by_op\": result.drops_by_op,\n    }\n    with open(path, \"w\") as f:\n        json.dump(baseline, f)\n    print(f\"Baseline saved to {path}\")\n\ndef compare_to_baseline(df, baseline_path=\"baseline_stats.json\"):\n    \"\"\"Compare current run to baseline\"\"\"\n    with open(baseline_path) as f:\n        baseline = json.load(f)\n\n    result = tp.check(df)\n\n    # Check for regressions\n    issues = []\n\n    retention_diff = result.retention - baseline[\"retention\"]\n    if retention_diff &lt; -0.05:  # 5% regression threshold\n        issues.append(\n            f\"Retention dropped: {baseline['retention']:.1%} \u2192 {result.retention:.1%}\"\n        )\n\n    row_diff = len(df) - baseline[\"n_rows\"]\n    if abs(row_diff) &gt; baseline[\"n_rows\"] * 0.1:  # 10% change\n        issues.append(\n            f\"Row count changed significantly: {baseline['n_rows']} \u2192 {len(df)}\"\n        )\n\n    if issues:\n        print(\"\u26a0\ufe0f Regressions detected:\")\n        for issue in issues:\n            print(f\"  \u2022 {issue}\")\n        return False\n\n    print(\"\u2713 No regressions detected\")\n    return True\n</code></pre>"},{"location":"examples/data-validation/#best-practices","title":"Best Practices","text":""},{"location":"examples/data-validation/#1-use-ci-mode-in-production","title":"1. Use CI Mode in Production","text":"<pre><code># Production: lightweight tracking\ntp.enable(mode=\"ci\")\n\n# Development: full debugging\ntp.enable(mode=\"debug\", watch=[\"important_col\"])\n</code></pre>"},{"location":"examples/data-validation/#2-set-appropriate-thresholds","title":"2. Set Appropriate Thresholds","text":"<pre><code># Strict for critical pipelines\n.expect_retention(min_rate=0.99)\n\n# Lenient for exploratory analysis\n.expect_retention(min_rate=0.5)\n</code></pre>"},{"location":"examples/data-validation/#3-layer-contracts","title":"3. Layer Contracts","text":"<pre><code># Schema first (fast fail)\ntp.contract().expect_columns([...]).check(df).raise_if_failed()\n\n# Then data quality\ntp.contract().expect_unique(...).expect_no_nulls(...).check(df).raise_if_failed()\n\n# Finally pipeline health\ntp.contract().expect_retention(...).check(df).raise_if_failed()\n</code></pre>"},{"location":"examples/data-validation/#4-document-expectations","title":"4. Document Expectations","text":"<pre><code># Clear contract with comments\ncontract = (tp.contract()\n    .expect_unique(\"order_id\")        # Business rule: no duplicate orders\n    .expect_no_nulls(\"customer_id\")   # Required for downstream joins\n    .expect_retention(min_rate=0.95)  # Historical average is 97%\n)\n</code></pre>"},{"location":"examples/ml-pipeline/","title":"ML Pipeline Example","text":"<p>A complete example of using TracePipe to audit an ML data pipeline.</p>"},{"location":"examples/ml-pipeline/#the-scenario","title":"The Scenario","text":"<p>You're building a customer churn prediction model. Your pipeline:</p> <ol> <li>Loads customer data</li> <li>Cleans missing values</li> <li>Filters valid customers</li> <li>Engineers features</li> <li>Merges with additional data</li> </ol> <p>You need to understand what happens to your training data.</p>"},{"location":"examples/ml-pipeline/#full-example","title":"Full Example","text":"<pre><code>import tracepipe as tp\nimport pandas as pd\nimport numpy as np\n\n# Enable debug tracking\ntp.enable(mode=\"debug\", watch=[\"income\", \"age\", \"churn_score\"])\n\n# =============================================================================\n# Stage 1: Load Data\n# =============================================================================\ntp.stage(\"load\")\n\ncustomers = pd.DataFrame({\n    \"customer_id\": [f\"C-{i}\" for i in range(1000)],\n    \"age\": np.random.randint(18, 80, 1000),\n    \"income\": np.random.normal(50000, 20000, 1000),\n    \"tenure_months\": np.random.randint(1, 120, 1000),\n    \"region\": np.random.choice([\"US\", \"EU\", \"APAC\"], 1000),\n    \"churn_score\": np.random.random(1000),\n})\n\n# Inject some realistic issues\ncustomers.loc[50:100, \"income\"] = None\ncustomers.loc[200:220, \"age\"] = -1  # Invalid ages\ncustomers.loc[300:310, \"churn_score\"] = None\n\nprint(f\"Loaded {len(customers)} customers\")\n\n# =============================================================================\n# Stage 2: Clean Data\n# =============================================================================\ntp.stage(\"clean\")\n\n# Drop rows with missing income\ncustomers = customers.dropna(subset=[\"income\"])\nprint(f\"After dropna: {len(customers)}\")\n\n# Fill missing churn scores with median\nmedian_score = customers[\"churn_score\"].median()\ncustomers[\"churn_score\"] = customers[\"churn_score\"].fillna(median_score)\n\n# Remove invalid ages\ncustomers = customers[customers[\"age\"] &gt; 0]\nprint(f\"After age filter: {len(customers)}\")\n\n# =============================================================================\n# Stage 3: Feature Engineering\n# =============================================================================\ntp.stage(\"features\")\n\n# Normalize income\ncustomers[\"income_normalized\"] = (\n    customers[\"income\"] - customers[\"income\"].mean()\n) / customers[\"income\"].std()\n\n# Age buckets\ncustomers[\"age_bucket\"] = pd.cut(\n    customers[\"age\"],\n    bins=[0, 25, 35, 50, 65, 100],\n    labels=[\"18-25\", \"26-35\", \"36-50\", \"51-65\", \"65+\"]\n)\n\n# Log transform\ncustomers[\"log_tenure\"] = np.log1p(customers[\"tenure_months\"])\n\n# =============================================================================\n# Stage 4: Enrich with Region Data\n# =============================================================================\ntp.stage(\"enrich\")\n\nregion_data = pd.DataFrame({\n    \"region\": [\"US\", \"EU\", \"APAC\"],\n    \"market_size\": [1.0, 0.8, 1.2],\n    \"growth_rate\": [0.05, 0.03, 0.08],\n})\n\ncustomers = customers.merge(region_data, on=\"region\", how=\"left\")\n\n# =============================================================================\n# Audit the Pipeline\n# =============================================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"PIPELINE AUDIT\")\nprint(\"=\"*60)\n\n# Health check\nresult = tp.check(customers)\nprint(result)\n\n# Contract validation\nprint(\"\\n--- Contract Check ---\")\ncontract_result = (tp.contract()\n    .expect_no_nulls([\"customer_id\", \"income\", \"churn_score\"])\n    .expect_unique(\"customer_id\")\n    .expect_retention(min_rate=0.8)\n    .expect_range(\"age\", min_val=0, max_val=150)\n    .check(customers))\n\nprint(contract_result)\n\n# Trace a specific customer\nprint(\"\\n--- Customer Journey ---\")\ntrace = tp.trace(customers, where={\"customer_id\": \"C-55\"})\nprint(trace)\n\n# Why did churn_score change?\nprint(\"\\n--- Cell Provenance ---\")\nwhy = tp.why(customers, col=\"churn_score\", where={\"customer_id\": \"C-305\"})\nprint(why)\n\n# Generate report\ntp.report(customers, \"ml_pipeline_audit.html\", title=\"Churn Model Pipeline Audit\")\nprint(\"\\n\u2713 Report saved to ml_pipeline_audit.html\")\n\n# Debug inspection\nprint(\"\\n--- Debug Stats ---\")\ndbg = tp.debug.inspect()\nstats = dbg.stats()\nprint(f\"Steps recorded: {stats['steps_recorded']}\")\nprint(f\"Rows dropped: {stats['rows_dropped']}\")\n\n# Export lineage\ndbg.export(\"json\", \"ml_pipeline_lineage.json\")\nprint(\"\u2713 Lineage exported to ml_pipeline_lineage.json\")\n\ntp.disable()\n</code></pre>"},{"location":"examples/ml-pipeline/#expected-output","title":"Expected Output","text":"<pre><code>Loaded 1000 customers\nAfter dropna: 949\nAfter age filter: 928\n\n============================================================\nPIPELINE AUDIT\n============================================================\nTracePipe Check: [OK] Pipeline healthy\n  Mode: debug\n\nRetention: 928/1000 (92.8%)\nDropped: 72 rows\n  \u2022 DataFrame.dropna: 51\n  \u2022 DataFrame.__getitem__[mask]: 21\n\nValue changes: 11 cells modified\n  \u2022 DataFrame.fillna: 11 (churn_score)\n\n--- Contract Check ---\nContract: [PASSED] All 5 expectations met\n  \u2713 no_nulls(customer_id, income, churn_score)\n  \u2713 unique(customer_id)\n  \u2713 retention &gt;= 80.0%\n  \u2713 range(age): 0 &lt;= x &lt;= 150\n\n--- Customer Journey ---\nRow 55 Journey:\n  Status: [DROPPED]\n  Dropped by: DataFrame.dropna (step 2)\n\n  Events: 1\n    [DROPPED] DataFrame.dropna\n\n--- Cell Provenance ---\nCell History: row 294, column 'churn_score'\n  Current value: 0.4821\n  [i] Was null at step 1 (later recovered)\n      by: DataFrame.fillna\n\n  History (1 change):\n    None -&gt; 0.4821\n      by: DataFrame.fillna\n\n\u2713 Report saved to ml_pipeline_audit.html\n\n--- Debug Stats ---\nSteps recorded: 12\nRows dropped: 72\n\u2713 Lineage exported to ml_pipeline_lineage.json\n</code></pre>"},{"location":"examples/ml-pipeline/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Visibility: You can see exactly where rows are dropped and why</li> <li>Null Recovery: TracePipe flags when nulls are filled, showing the imputation</li> <li>Contract Validation: Ensure your training data meets quality standards</li> <li>Reproducibility: Export lineage for documentation and debugging</li> </ol>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.9 or higher</li> <li>pandas 1.5.0 or higher</li> </ul>"},{"location":"getting-started/installation/#basic-installation","title":"Basic Installation","text":"<p>Install TracePipe from PyPI:</p> <pre><code>pip install tracepipe\n</code></pre>"},{"location":"getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":"<p>TracePipe has optional dependencies for additional features:</p> Arrow/Parquet SupportMemory ProfilingAll Features <pre><code>pip install tracepipe[arrow]\n</code></pre> <p>Enables:</p> <ul> <li><code>pd.read_parquet()</code> tracking</li> <li><code>df.to_parquet()</code> with automatic column stripping</li> <li>Optimized Arrow-based serialization</li> </ul> <pre><code>pip install tracepipe[memory]\n</code></pre> <p>Enables:</p> <ul> <li>Memory usage statistics in <code>tp.debug.inspect().stats()</code></li> <li>Process memory tracking</li> </ul> <pre><code>pip install tracepipe[all]\n</code></pre> <p>Installs all optional dependencies.</p>"},{"location":"getting-started/installation/#development-installation","title":"Development Installation","text":"<p>For contributing to TracePipe:</p> <pre><code>git clone https://github.com/tracepipe/tracepipe.git\ncd tracepipe\n\n# Using uv (recommended)\nuv sync --all-extras\n\n# Or using pip\npip install -e \".[dev]\"\n</code></pre>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<pre><code>import tracepipe as tp\nprint(tp.__version__)  # Should print version number\n\n# Quick test\nimport pandas as pd\ntp.enable()\ndf = pd.DataFrame({\"a\": [1, 2, 3]})\ndf = df.dropna()\nprint(tp.check(df))\ntp.disable()\n</code></pre>"},{"location":"getting-started/installation/#supported-pandas-versions","title":"Supported Pandas Versions","text":"<p>TracePipe is tested against:</p> pandas Version Status 1.5.x \u2705 Supported 2.0.x \u2705 Supported 2.1.x \u2705 Supported 2.2.x \u2705 Supported"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#import-error","title":"Import Error","text":"<p>If you get an import error, ensure pandas is installed:</p> <pre><code>pip install pandas&gt;=1.5.0\n</code></pre>"},{"location":"getting-started/installation/#version-conflicts","title":"Version Conflicts","text":"<p>If you have version conflicts, try creating a fresh virtual environment:</p> <pre><code>python -m venv .venv\nsource .venv/bin/activate  # Linux/Mac\n# or: .venv\\Scripts\\activate  # Windows\npip install tracepipe\n</code></pre>"},{"location":"getting-started/modes/","title":"Tracking Modes","text":"<p>TracePipe offers two tracking modes optimized for different use cases.</p>"},{"location":"getting-started/modes/#ci-mode-default","title":"CI Mode (Default)","text":"<p>Lightweight tracking for production and CI/CD pipelines.</p> <pre><code>tp.enable(mode=\"ci\")\n</code></pre>"},{"location":"getting-started/modes/#whats-tracked","title":"What's Tracked","text":"<ul> <li>\u2705 Step counts and shapes</li> <li>\u2705 Retention rates</li> <li>\u2705 Dropped row counts (not individual IDs)</li> <li>\u2705 Merge mismatch warnings</li> <li>\u274c Per-row provenance</li> <li>\u274c Cell change history</li> <li>\u274c Ghost values (last values before drop)</li> </ul>"},{"location":"getting-started/modes/#when-to-use","title":"When to Use","text":"<ul> <li>Production data pipelines</li> <li>CI/CD validation</li> <li>Performance-critical code</li> <li>Large datasets (1M+ rows)</li> </ul>"},{"location":"getting-started/modes/#performance","title":"Performance","text":"<p>Minimal overhead \u2014 typically &lt; 10% slower than untracked pandas.</p>"},{"location":"getting-started/modes/#debug-mode","title":"Debug Mode","text":"<p>Full lineage tracking for development and debugging.</p> <pre><code>tp.enable(mode=\"debug\", watch=[\"price\", \"amount\", \"status\"])\n</code></pre>"},{"location":"getting-started/modes/#whats-tracked_1","title":"What's Tracked","text":"<ul> <li>\u2705 Everything in CI mode, plus:</li> <li>\u2705 Individual dropped row IDs</li> <li>\u2705 Complete row-level history</li> <li>\u2705 Cell change tracking (for watched columns)</li> <li>\u2705 Before/after values</li> <li>\u2705 Ghost values (captured at drop time)</li> <li>\u2705 Merge parent tracking</li> <li>\u2705 GroupBy membership</li> </ul>"},{"location":"getting-started/modes/#when-to-use_1","title":"When to Use","text":"<ul> <li>Debugging data issues</li> <li>Understanding pipeline behavior</li> <li>Auditing transformations</li> <li>Investigating specific rows</li> </ul>"},{"location":"getting-started/modes/#performance_1","title":"Performance","text":"<p>Higher overhead due to per-row tracking. Expect 1.5-3x slower depending on operations.</p>"},{"location":"getting-started/modes/#the-watch-parameter","title":"The <code>watch</code> Parameter","text":"<p>In debug mode, specify which columns to track for cell-level changes:</p> <pre><code>tp.enable(mode=\"debug\", watch=[\"price\", \"quantity\", \"status\"])\n</code></pre> <p>Watch Strategy</p> <p>Only watch columns you care about. Watching all columns significantly increases memory usage and overhead.</p>"},{"location":"getting-started/modes/#what-watching-enables","title":"What Watching Enables","text":"Feature Without Watch With Watch <code>tp.check()</code> retention \u2705 \u2705 <code>tp.trace()</code> row journey \u2705 \u2705 <code>tp.why()</code> cell history \u274c \u2705 Before/after values \u274c \u2705 Ghost values \u274c \u2705"},{"location":"getting-started/modes/#switching-modes","title":"Switching Modes","text":"<p>You can switch modes at any time:</p> <pre><code># Start in CI mode for initial load\ntp.enable(mode=\"ci\")\ndf = pd.read_csv(\"large_file.csv\")\n\n# Switch to debug for specific analysis\ntp.reset()  # Clear previous lineage\ntp.enable(mode=\"debug\", watch=[\"target_column\"])\ndf = df[df[\"flag\"] == True]  # Now tracked in detail\n</code></pre>"},{"location":"getting-started/modes/#mode-comparison","title":"Mode Comparison","text":"Feature CI Mode Debug Mode Memory usage Low Medium-High Performance overhead &lt; 10% 50-200% <code>tp.check()</code> Full Full <code>tp.trace()</code> Limited Full <code>tp.why()</code> Not available Full Ghost values No Yes Merge parents Counts only Full tracking"},{"location":"getting-started/modes/#configuration","title":"Configuration","text":"<p>Additional configuration options:</p> <pre><code>tp.enable(\n    mode=\"debug\",\n    watch=[\"col1\", \"col2\"],\n    # Advanced options via configure()\n)\n\n# Or use configure() separately\ntp.configure(\n    sample_rate=0.1,      # Sample 10% of rows (for huge datasets)\n    max_tracked_rows=100000,  # Cap tracked rows\n)\n</code></pre> <p>Note</p> <p><code>sample_rate</code> and <code>max_tracked_rows</code> are planned features for handling very large datasets efficiently.</p>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>Get up and running with TracePipe in 5 minutes.</p>"},{"location":"getting-started/quickstart/#1-enable-tracking","title":"1. Enable Tracking","text":"<pre><code>import tracepipe as tp\nimport pandas as pd\n\n# Start tracking with debug mode for full lineage\ntp.enable(mode=\"debug\", watch=[\"price\", \"quantity\"])\n</code></pre> <p>The <code>watch</code> parameter specifies which columns to track for cell-level changes.</p>"},{"location":"getting-started/quickstart/#2-run-your-pipeline","title":"2. Run Your Pipeline","text":"<p>Write your pandas code as usual \u2014 TracePipe instruments it automatically:</p> <pre><code>df = pd.DataFrame({\n    \"product\": [\"A\", \"B\", \"C\", \"D\"],\n    \"price\": [10.0, None, 30.0, 40.0],\n    \"quantity\": [5, 10, 0, 8]\n})\n\ndf = df.dropna()                    # Drops row B (null price)\ndf = df[df[\"quantity\"] &gt; 0]         # Drops row C (zero quantity)\ndf[\"total\"] = df[\"price\"] * df[\"quantity\"]\n</code></pre>"},{"location":"getting-started/quickstart/#3-check-pipeline-health","title":"3. Check Pipeline Health","text":"<pre><code>result = tp.check(df)\nprint(result)\n</code></pre> <p>Output:</p> <pre><code>TracePipe Check: [OK] Pipeline healthy\n  Mode: debug\n\nRetention: 2/4 (50.0%)\nDropped: 2 rows\n  \u2022 DataFrame.dropna: 1\n  \u2022 DataFrame.__getitem__[mask]: 1\n\nValue changes: 2 cells\n  \u2022 DataFrame.__setitem__[total]: 2\n</code></pre>"},{"location":"getting-started/quickstart/#4-trace-a-rows-journey","title":"4. Trace a Row's Journey","text":"<pre><code>trace = tp.trace(df, where={\"product\": \"A\"})\nprint(trace)\n</code></pre> <p>Output:</p> <pre><code>Row 0 Journey:\n  Status: [OK] Alive\n\n  Events: 1\n    [MODIFIED] DataFrame.__setitem__[total]: total\n</code></pre>"},{"location":"getting-started/quickstart/#5-explain-a-cells-value","title":"5. Explain a Cell's Value","text":"<pre><code>why = tp.why(df, col=\"total\", row=0)\nprint(why)\n</code></pre> <p>Output:</p> <pre><code>Cell History: row 0, column 'total'\n  Current value: 50.0\n\n  History (1 change):\n    None -&gt; 50.0\n      by: DataFrame.__setitem__[total]\n</code></pre>"},{"location":"getting-started/quickstart/#6-generate-a-report","title":"6. Generate a Report","text":"<pre><code>tp.report(df, \"pipeline_audit.html\")\n</code></pre> <p>This creates an interactive HTML report with:</p> <ul> <li>Pipeline flow diagram</li> <li>Retention funnel visualization</li> <li>Dropped rows by operation</li> <li>Cell change history</li> </ul>"},{"location":"getting-started/quickstart/#7-clean-up","title":"7. Clean Up","text":"<pre><code>tp.disable()  # Stop tracking\ntp.reset()    # Clear all lineage data\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about CI vs Debug modes</li> <li>Explore health checks in depth</li> <li>Set up data contracts</li> <li>See real-world examples</li> </ul>"},{"location":"guide/cell-provenance/","title":"Cell Provenance","text":"<p>Understand why a specific cell has its current value.</p>"},{"location":"guide/cell-provenance/#basic-usage","title":"Basic Usage","text":"<pre><code># By row index and column\nwhy = tp.why(df, col=\"income\", row=0)\nprint(why)\n\n# By business key\nwhy = tp.why(df, col=\"income\", where={\"customer_id\": \"C-12345\"})\nprint(why)\n</code></pre> <p>Output:</p> <pre><code>Cell History: row 42, column 'income'\n  Current value: 45000.0\n  [i] Was null at step 1 (later recovered)\n      by: DataFrame.fillna\n\n  History (1 change):\n    None -&gt; 45000.0\n      by: DataFrame.fillna\n</code></pre>"},{"location":"guide/cell-provenance/#the-whyresult-object","title":"The WhyResult Object","text":"<pre><code>why = tp.why(df, col=\"income\", row=0)\n\n# Access fields\nwhy.column           # str: column name\nwhy.row_id           # int: internal row ID\nwhy.current_value    # any: current cell value\nwhy.history          # list[CellChange]: all changes\n\n# Flags\nwhy.was_null         # bool: was ever null\nwhy.null_recovered   # bool: null was later filled\n\n# Export\nwhy.to_dict()        # dict representation\n</code></pre>"},{"location":"guide/cell-provenance/#cell-change-records","title":"Cell Change Records","text":"<p>Each change in history contains:</p> <pre><code>for change in why.history:\n    print(f\"From: {change.old_value}\")\n    print(f\"To: {change.new_value}\")\n    print(f\"By: {change.operation}\")\n    print(f\"At step: {change.step}\")\n</code></pre>"},{"location":"guide/cell-provenance/#requirements","title":"Requirements","text":"<p>Debug Mode Required</p> <p><code>tp.why()</code> requires debug mode with the column being watched:</p> <pre><code>tp.enable(mode=\"debug\", watch=[\"income\", \"status\"])\n</code></pre> <p>Columns not in <code>watch</code> will not have cell history.</p>"},{"location":"guide/cell-provenance/#common-use-cases","title":"Common Use Cases","text":""},{"location":"guide/cell-provenance/#finding-null-introduction","title":"Finding Null Introduction","text":"<pre><code># Where did this null come from?\nwhy = tp.why(df, col=\"email\", where={\"email\": None})\n\nif why.was_null:\n    print(\"This cell was null from the start\")\nelse:\n    # Find which operation set it to null\n    for change in why.history:\n        if change.new_value is None:\n            print(f\"Null introduced by: {change.operation}\")\n</code></pre>"},{"location":"guide/cell-provenance/#tracking-value-changes","title":"Tracking Value Changes","text":"<pre><code># How did this price get so high?\nwhy = tp.why(df, col=\"price\", row=0)\n\nfor change in why.history:\n    pct_change = (change.new_value - change.old_value) / change.old_value * 100\n    print(f\"{change.operation}: {pct_change:+.1f}%\")\n</code></pre>"},{"location":"guide/cell-provenance/#auditing-sensitive-fields","title":"Auditing Sensitive Fields","text":"<pre><code># Who touched the salary column?\nfor idx in range(len(df)):\n    why = tp.why(df, col=\"salary\", row=idx)\n    if why.history:\n        print(f\"Row {idx}: {len(why.history)} changes\")\n</code></pre>"},{"location":"guide/cell-provenance/#finding-cells","title":"Finding Cells","text":""},{"location":"guide/cell-provenance/#by-index","title":"By Index","text":"<pre><code>tp.why(df, col=\"price\", row=0)\ntp.why(df, col=\"price\", row=-1)  # Last row\n</code></pre>"},{"location":"guide/cell-provenance/#by-business-key","title":"By Business Key","text":"<pre><code>tp.why(df, col=\"status\", where={\"order_id\": \"ORD-123\"})\n</code></pre>"},{"location":"guide/cell-provenance/#multiple-matches","title":"Multiple Matches","text":"<p>If <code>where=</code> matches multiple rows, TracePipe returns the first match. For multiple rows, iterate:</p> <pre><code>matching_rows = df[df[\"region\"] == \"US\"]\nfor idx in range(len(matching_rows)):\n    why = tp.why(matching_rows, col=\"price\", row=idx)\n    # Process each\n</code></pre>"},{"location":"guide/cell-provenance/#ghost-values","title":"Ghost Values","text":"<p>For dropped rows, you can still query their last known values:</p> <pre><code>dbg = tp.debug.inspect()\n\n# Get ghost values for a dropped row\ndropped_rid = list(dbg.dropped_rows())[0]\nghost = dbg.get_ghost_values(dropped_rid)\n\nprint(f\"Last known values: {ghost}\")\n</code></pre>"},{"location":"guide/concepts/","title":"Core Concepts","text":"<p>Understanding how TracePipe tracks your data.</p>"},{"location":"guide/concepts/#row-identity","title":"Row Identity","text":"<p>TracePipe assigns a unique ID to every row when it first appears in your pipeline. This ID persists through transformations:</p> <pre><code>tp.enable(mode=\"debug\")\n\ndf = pd.DataFrame({\"a\": [1, 2, 3]})  # Rows get IDs: 0, 1, 2\ndf = df[df[\"a\"] &gt; 1]                  # Row 0 dropped, IDs 1, 2 remain\ndf[\"b\"] = df[\"a\"] * 2                 # IDs unchanged\n</code></pre>"},{"location":"guide/concepts/#how-ids-are-stored","title":"How IDs Are Stored","text":"<p>TracePipe uses one of three storage strategies:</p> Strategy Description Use Case WeakRef (default) IDs stored in a weak reference dictionary Most pipelines Attrs IDs stored in <code>df.attrs</code> Long-running sessions Column IDs stored as a hidden <code>__tracepipe_row_id__</code> column Maximum reliability <p>The hidden column is automatically stripped when exporting with <code>to_csv()</code> or <code>to_parquet()</code>.</p>"},{"location":"guide/concepts/#lineage-events","title":"Lineage Events","text":"<p>TracePipe records several types of events:</p>"},{"location":"guide/concepts/#drop-events","title":"Drop Events","text":"<p>When rows are removed from a DataFrame:</p> <pre><code>df = df.dropna()           # DROP event recorded\ndf = df[df[\"age\"] &gt;= 18]   # DROP event recorded\ndf = df.head(100)          # DROP event recorded\n</code></pre>"},{"location":"guide/concepts/#transform-events","title":"Transform Events","text":"<p>When cell values change:</p> <pre><code>df[\"price\"] = df[\"price\"] * 1.1    # TRANSFORM event (if \"price\" is watched)\ndf.loc[0, \"status\"] = \"active\"     # TRANSFORM event (if \"status\" is watched)\ndf[\"income\"] = df[\"income\"].fillna(0)  # TRANSFORM event\n</code></pre>"},{"location":"guide/concepts/#merge-events","title":"Merge Events","text":"<p>When rows are combined from multiple DataFrames:</p> <pre><code>result = df1.merge(df2, on=\"id\")  # MERGE event with parent tracking\n</code></pre>"},{"location":"guide/concepts/#group-events","title":"Group Events","text":"<p>When rows are aggregated:</p> <pre><code>grouped = df.groupby(\"category\").sum()  # GROUP event with membership\n</code></pre>"},{"location":"guide/concepts/#the-lineage-store","title":"The Lineage Store","text":"<p>All events are stored in an in-memory lineage store. You can inspect it directly:</p> <pre><code>dbg = tp.debug.inspect()\n\n# View all steps\nfor step in dbg.steps:\n    print(f\"{step.operation}: {step.input_shape} -&gt; {step.output_shape}\")\n\n# View dropped rows\nprint(f\"Total dropped: {len(dbg.dropped_rows())}\")\n\n# Export raw lineage\ndbg.export(\"json\", \"lineage.json\")\n</code></pre>"},{"location":"guide/concepts/#ghost-values","title":"Ghost Values","text":"<p>In debug mode, TracePipe captures the last known values of dropped rows:</p> <pre><code>tp.enable(mode=\"debug\", watch=[\"email\", \"status\"])\n\ndf = pd.DataFrame({\n    \"email\": [\"a@x.com\", \"b@x.com\", None],\n    \"status\": [\"active\", \"inactive\", \"pending\"]\n})\ndf = df.dropna()  # Row 2 dropped, but its values are captured\n\n# Ghost values are available in reports and debug.inspect()\n</code></pre> <p>This helps answer \"what was the value of X when it was dropped?\"</p>"},{"location":"guide/concepts/#stages","title":"Stages","text":"<p>Label pipeline stages for better organization:</p> <pre><code>tp.stage(\"load\")\ndf = pd.read_csv(\"data.csv\")\n\ntp.stage(\"clean\")\ndf = df.dropna()\ndf = df.drop_duplicates()\n\ntp.stage(\"transform\")\ndf[\"total\"] = df[\"price\"] * df[\"quantity\"]\n\n# Stages appear in reports and check output\nprint(tp.check(df))\n</code></pre>"},{"location":"guide/concepts/#retention-rate","title":"Retention Rate","text":"<p>TracePipe tracks how many rows survive through your pipeline:</p> <pre><code>Retention = Final Rows / Maximum Rows Seen\n</code></pre> <p>Multi-table Pipelines</p> <p>For pipelines with merges, TracePipe uses the maximum row count seen across all steps as the baseline, accounting for row expansion from joins.</p> <p>A retention rate below 50% triggers a warning by default. Configure this threshold:</p> <pre><code>result = tp.check(df, retention_threshold=0.3)  # Warn below 30%\n</code></pre>"},{"location":"guide/contracts/","title":"Data Contracts","text":"<p>Define and validate expectations on your pipeline output.</p>"},{"location":"guide/contracts/#basic-usage","title":"Basic Usage","text":"<pre><code>result = (tp.contract()\n    .expect_unique(\"customer_id\")\n    .expect_no_nulls(\"email\")\n    .expect_retention(min_rate=0.8)\n    .check(df))\n\nprint(result)\n</code></pre> <p>Output (if passing):</p> <pre><code>Contract: [PASSED] All 3 expectations met\n  \u2713 unique(customer_id)\n  \u2713 no_nulls(email)\n  \u2713 retention &gt;= 80.0%\n</code></pre> <p>Output (if failing):</p> <pre><code>Contract: [FAILED] 1 of 3 expectations failed\n  \u2713 unique(customer_id)\n  \u2717 no_nulls(email): 5 nulls found\n  \u2713 retention &gt;= 80.0%\n</code></pre>"},{"location":"guide/contracts/#the-contractresult-object","title":"The ContractResult Object","text":"<pre><code>result = tp.contract().expect_unique(\"id\").check(df)\n\n# Access fields\nresult.passed          # bool: all expectations met\nresult.expectations    # list[Expectation]: all expectations\nresult.failures        # list[Expectation]: failed expectations\n\n# Raise on failure\nresult.raise_if_failed()  # Raises ContractViolationError\n</code></pre>"},{"location":"guide/contracts/#available-expectations","title":"Available Expectations","text":""},{"location":"guide/contracts/#expect_uniquecolumn","title":"<code>expect_unique(column)</code>","text":"<p>Ensures no duplicate values in a column:</p> <pre><code>.expect_unique(\"order_id\")\n.expect_unique(\"email\")\n</code></pre>"},{"location":"guide/contracts/#expect_no_nullscolumn","title":"<code>expect_no_nulls(column)</code>","text":"<p>Ensures no null values:</p> <pre><code>.expect_no_nulls(\"customer_id\")\n.expect_no_nulls([\"name\", \"email\"])  # Multiple columns\n</code></pre>"},{"location":"guide/contracts/#expect_retentionmin_rate","title":"<code>expect_retention(min_rate)</code>","text":"<p>Ensures minimum row retention:</p> <pre><code>.expect_retention(min_rate=0.9)   # At least 90% retained\n.expect_retention(min_rate=0.5)   # At least 50% retained\n</code></pre>"},{"location":"guide/contracts/#expect_no_drops","title":"<code>expect_no_drops()</code>","text":"<p>Ensures no rows were dropped:</p> <pre><code>.expect_no_drops()  # Fails if any row was dropped\n</code></pre>"},{"location":"guide/contracts/#expect_columnscolumns","title":"<code>expect_columns(columns)</code>","text":"<p>Ensures specific columns exist:</p> <pre><code>.expect_columns([\"id\", \"name\", \"email\"])\n</code></pre>"},{"location":"guide/contracts/#expect_dtypesdtypes","title":"<code>expect_dtypes(dtypes)</code>","text":"<p>Ensures column data types:</p> <pre><code>.expect_dtypes({\n    \"id\": \"int64\",\n    \"price\": \"float64\",\n    \"name\": \"object\"\n})\n</code></pre>"},{"location":"guide/contracts/#expect_rangecolumn-min_val-max_val","title":"<code>expect_range(column, min_val, max_val)</code>","text":"<p>Ensures values are within a range:</p> <pre><code>.expect_range(\"age\", min_val=0, max_val=150)\n.expect_range(\"price\", min_val=0)  # Just minimum\n</code></pre>"},{"location":"guide/contracts/#expect_valuescolumn-allowed","title":"<code>expect_values(column, allowed)</code>","text":"<p>Ensures values are from an allowed set:</p> <pre><code>.expect_values(\"status\", [\"active\", \"inactive\", \"pending\"])\n.expect_values(\"country\", [\"US\", \"CA\", \"UK\", \"DE\"])\n</code></pre>"},{"location":"guide/contracts/#chaining-expectations","title":"Chaining Expectations","text":"<p>Build complex contracts with method chaining:</p> <pre><code>contract = (tp.contract()\n    # Schema validation\n    .expect_columns([\"id\", \"email\", \"status\", \"amount\"])\n    .expect_dtypes({\"amount\": \"float64\"})\n\n    # Data quality\n    .expect_unique(\"id\")\n    .expect_no_nulls([\"id\", \"email\"])\n    .expect_values(\"status\", [\"active\", \"inactive\"])\n    .expect_range(\"amount\", min_val=0)\n\n    # Pipeline health\n    .expect_retention(min_rate=0.8)\n)\n\nresult = contract.check(df)\n</code></pre>"},{"location":"guide/contracts/#using-in-cicd","title":"Using in CI/CD","text":"<pre><code>import sys\n\nresult = (tp.contract()\n    .expect_unique(\"id\")\n    .expect_retention(min_rate=0.9)\n    .check(df))\n\nif not result.passed:\n    print(\"Data contract violated!\")\n    for failure in result.failures:\n        print(f\"  \u2717 {failure}\")\n    sys.exit(1)\n</code></pre> <p>Or use the exception-based approach:</p> <pre><code>try:\n    (tp.contract()\n        .expect_unique(\"id\")\n        .expect_retention(min_rate=0.9)\n        .check(df)\n        .raise_if_failed())\nexcept tp.ContractViolationError as e:\n    print(f\"Contract failed: {e}\")\n    sys.exit(1)\n</code></pre>"},{"location":"guide/contracts/#custom-expectations","title":"Custom Expectations","text":"<p>For custom validation logic:</p> <pre><code>def validate_email_format(df):\n    \"\"\"Check that all emails contain @\"\"\"\n    invalid = df[~df[\"email\"].str.contains(\"@\", na=False)]\n    if len(invalid) &gt; 0:\n        return False, f\"{len(invalid)} invalid emails\"\n    return True, None\n\n# Use with expect_custom (if available) or validate manually\nresult = tp.contract().expect_no_nulls(\"email\").check(df)\nif result.passed:\n    valid, msg = validate_email_format(df)\n    if not valid:\n        print(f\"Custom validation failed: {msg}\")\n</code></pre>"},{"location":"guide/health-checks/","title":"Health Checks","text":"<p>The <code>tp.check()</code> function provides a comprehensive health audit of your pipeline.</p>"},{"location":"guide/health-checks/#basic-usage","title":"Basic Usage","text":"<pre><code>result = tp.check(df)\nprint(result)\n</code></pre> <p>Output:</p> <pre><code>TracePipe Check: [OK] Pipeline healthy\n  Mode: debug\n\nRetention: 847/1000 (84.7%)\nDropped: 153 rows\n  \u2022 DataFrame.dropna: 42\n  \u2022 DataFrame.__getitem__[mask]: 111\n\nValue changes: 23 cells modified\n  \u2022 DataFrame.fillna: 23 (income)\n</code></pre>"},{"location":"guide/health-checks/#the-checkresult-object","title":"The CheckResult Object","text":"<p><code>tp.check()</code> returns a <code>CheckResult</code> object with programmatic access:</p> <pre><code>result = tp.check(df)\n\n# Access fields\nresult.passed          # bool: True if pipeline is healthy\nresult.mode            # str: \"ci\" or \"debug\"\nresult.retention       # float: 0.0-1.0 retention rate\nresult.n_dropped       # int: total dropped rows\nresult.n_changes       # int: total cell changes\nresult.warnings        # list[str]: any warnings\n\n# Breakdown by operation\nresult.drops_by_op     # dict: {operation: count}\nresult.changes_by_op   # dict: {operation: count}\n\n# Export\nresult.to_dict()       # dict representation\n</code></pre>"},{"location":"guide/health-checks/#warnings","title":"Warnings","text":"<p>TracePipe warns about common issues:</p>"},{"location":"guide/health-checks/#low-retention","title":"Low Retention","text":"<pre><code>\u26a0 Low retention (45.0%) \u2014 more than half of rows dropped\n</code></pre> <p>Triggered when retention falls below 50% (configurable).</p>"},{"location":"guide/health-checks/#merge-expansion","title":"Merge Expansion","text":"<pre><code>\u26a0 Merge expansion detected: 1000 \u2192 1500 rows (50% increase)\n</code></pre> <p>Triggered when a merge produces significantly more rows than input.</p>"},{"location":"guide/health-checks/#no-lineage","title":"No Lineage","text":"<pre><code>\u26a0 No lineage recorded \u2014 did you call tp.enable()?\n</code></pre> <p>Triggered when checking a DataFrame with no tracking data.</p>"},{"location":"guide/health-checks/#configuration","title":"Configuration","text":"<pre><code># Custom thresholds\nresult = tp.check(\n    df,\n    retention_threshold=0.3,      # Warn below 30%\n    merge_expansion_threshold=2.0  # Warn if merge doubles rows\n)\n</code></pre>"},{"location":"guide/health-checks/#using-in-cicd","title":"Using in CI/CD","text":"<pre><code>result = tp.check(df)\n\nif not result.passed:\n    print(\"Pipeline health check failed!\")\n    for warning in result.warnings:\n        print(f\"  \u2022 {warning}\")\n    sys.exit(1)\n</code></pre> <p>Or use contracts for stricter validation:</p> <pre><code>tp.contract().expect_retention(min_rate=0.8).check(df).raise_if_failed()\n</code></pre>"},{"location":"guide/health-checks/#check-vs-contracts","title":"Check vs Contracts","text":"Feature <code>tp.check()</code> <code>tp.contract()</code> Purpose Quick health audit Strict validation Failure behavior Returns result Can raise exception Customization Thresholds Full contract DSL Use case Development, monitoring CI/CD gates"},{"location":"guide/reports/","title":"HTML Reports","text":"<p>Generate interactive visual reports of your pipeline.</p>"},{"location":"guide/reports/#basic-usage","title":"Basic Usage","text":"<pre><code>tp.report(df, \"pipeline_audit.html\")\n</code></pre> <p>This creates a standalone HTML file with:</p> <ul> <li>Pipeline flow diagram</li> <li>Retention funnel visualization</li> <li>Dropped rows breakdown</li> <li>Cell change history</li> <li>Interactive filtering</li> </ul>"},{"location":"guide/reports/#report-contents","title":"Report Contents","text":""},{"location":"guide/reports/#pipeline-overview","title":"Pipeline Overview","text":"<p>Shows high-level statistics:</p> <ul> <li>Total rows processed</li> <li>Final row count</li> <li>Overall retention rate</li> <li>Number of steps</li> </ul>"},{"location":"guide/reports/#retention-funnel","title":"Retention Funnel","text":"<p>Visual representation of how rows flow through each step:</p> <pre><code>Load:     \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 1000 rows\ndropna:   \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591  850 rows (-150)\nfilter:   \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591  600 rows (-250)\nmerge:    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591  900 rows (+300)\nfinal:    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591  847 rows (-53)\n</code></pre>"},{"location":"guide/reports/#drops-by-operation","title":"Drops by Operation","text":"<p>Breakdown of which operations dropped the most rows:</p> Operation Rows Dropped % of Total DataFrame.dropna 150 37% DataFrame.getitem[mask] 250 62% DataFrame.drop_duplicates 3 1%"},{"location":"guide/reports/#cell-changes","title":"Cell Changes","text":"<p>For watched columns, shows modification history:</p> Column Changes Operations income 423 DataFrame.fillna status 89 DataFrame.setitem"},{"location":"guide/reports/#ghost-values","title":"Ghost Values","text":"<p>Last known values of dropped rows (debug mode only):</p> Row ID email status Dropped By 42 alice@... active dropna 156 bob@... inactive filter"},{"location":"guide/reports/#options","title":"Options","text":""},{"location":"guide/reports/#custom-title","title":"Custom Title","text":"<pre><code>tp.report(df, \"audit.html\", title=\"Customer Pipeline - Q4 2024\")\n</code></pre>"},{"location":"guide/reports/#include-raw-data","title":"Include Raw Data","text":"<pre><code>tp.report(df, \"audit.html\", include_data=True)\n</code></pre> <p>Adds a data preview table to the report. Use with caution for large DataFrames.</p>"},{"location":"guide/reports/#minimal-report","title":"Minimal Report","text":"<pre><code>tp.report(df, \"audit.html\", minimal=True)\n</code></pre> <p>Generates a simpler report without charts (faster, smaller file size).</p>"},{"location":"guide/reports/#programmatic-access","title":"Programmatic Access","text":"<p>If you need the report data without HTML:</p> <pre><code># Get report data as dict\ndbg = tp.debug.inspect()\nreport_data = dbg.export(\"dict\")\n\n# Contains:\n# - steps: list of all operations\n# - drops: dropped row details\n# - changes: cell modifications\n# - stats: summary statistics\n</code></pre>"},{"location":"guide/reports/#viewing-reports","title":"Viewing Reports","text":"<p>The generated HTML is self-contained (no external dependencies). Open it in any browser:</p> <pre><code># macOS\nopen pipeline_audit.html\n\n# Linux\nxdg-open pipeline_audit.html\n\n# Windows\nstart pipeline_audit.html\n</code></pre>"},{"location":"guide/reports/#integration-with-notebooks","title":"Integration with Notebooks","text":"<p>In Jupyter notebooks, you can display reports inline:</p> <pre><code>from IPython.display import HTML\n\ntp.report(df, \"audit.html\")\nHTML(\"audit.html\")\n</code></pre> <p>Or use the display helper:</p> <pre><code># If available\ntp.display_report(df)  # Renders inline in notebook\n</code></pre>"},{"location":"guide/reports/#example-report","title":"Example Report","text":"<pre><code>import tracepipe as tp\nimport pandas as pd\n\ntp.enable(mode=\"debug\", watch=[\"income\", \"status\"])\n\n# Sample pipeline\ndf = pd.read_csv(\"customers.csv\")\ndf = df.dropna(subset=[\"email\"])\ndf[\"income\"] = df[\"income\"].fillna(df[\"income\"].median())\ndf = df[df[\"age\"] &gt;= 18]\ndf = df.merge(segments, on=\"segment_id\")\n\n# Generate comprehensive report\ntp.report(df, \"customer_pipeline_audit.html\", title=\"Customer ETL Pipeline\")\n</code></pre>"},{"location":"guide/row-tracing/","title":"Row Tracing","text":"<p>Trace the complete journey of any row through your pipeline.</p>"},{"location":"guide/row-tracing/#basic-usage","title":"Basic Usage","text":"<pre><code># By row index\ntrace = tp.trace(df, row=0)\nprint(trace)\n\n# By business key\ntrace = tp.trace(df, where={\"customer_id\": \"C-12345\"})\nprint(trace)\n</code></pre> <p>Output:</p> <pre><code>Row 42 Journey:\n  Status: [OK] Alive\n\n  Events: 3\n    [SURVIVED] DataFrame.dropna\n    [MODIFIED] DataFrame.fillna: income\n    [SURVIVED] DataFrame.__getitem__[mask]\n</code></pre>"},{"location":"guide/row-tracing/#the-traceresult-object","title":"The TraceResult Object","text":"<pre><code>trace = tp.trace(df, row=0)\n\n# Access fields\ntrace.row_id           # int: internal row ID\ntrace.status           # str: \"alive\" or \"dropped\"\ntrace.events           # list[TraceEvent]: all events\n\n# For dropped rows\ntrace.dropped_by       # str: operation that dropped the row\ntrace.dropped_at_step  # int: step number\n\n# Export\ntrace.to_dict()        # dict representation\n</code></pre>"},{"location":"guide/row-tracing/#finding-rows","title":"Finding Rows","text":""},{"location":"guide/row-tracing/#by-index","title":"By Index","text":"<pre><code># Current DataFrame index\ntp.trace(df, row=0)      # First row in current df\ntp.trace(df, row=-1)     # Last row in current df\n</code></pre>"},{"location":"guide/row-tracing/#by-business-key","title":"By Business Key","text":"<pre><code># Single key\ntp.trace(df, where={\"email\": \"alice@example.com\"})\n\n# Multiple keys (AND condition)\ntp.trace(df, where={\"region\": \"US\", \"status\": \"active\"})\n\n# Find row with null value\ntp.trace(df, where={\"email\": None})\n</code></pre> <p>Use Business Keys</p> <p>Business keys are more stable than row indices, which change as rows are filtered.</p>"},{"location":"guide/row-tracing/#event-types","title":"Event Types","text":"Event Type Description <code>SURVIVED</code> Row passed through operation unchanged <code>MODIFIED</code> One or more cells changed <code>DROPPED</code> Row was removed <code>CREATED</code> Row first appeared (e.g., from merge)"},{"location":"guide/row-tracing/#tracing-dropped-rows","title":"Tracing Dropped Rows","text":"<p>You can trace rows that were dropped:</p> <pre><code>dbg = tp.debug.inspect()\n\n# Get IDs of dropped rows\ndropped_ids = dbg.dropped_rows()\n\n# Trace a specific dropped row\nfor rid in list(dropped_ids)[:5]:\n    trace = dbg.explain_row(rid)\n    print(f\"Row {rid}: dropped by {trace.dropped_by}\")\n</code></pre>"},{"location":"guide/row-tracing/#merge-parent-tracking","title":"Merge Parent Tracking","text":"<p>For rows created by merges, TracePipe tracks their parents:</p> <pre><code>result = df1.merge(df2, on=\"id\")\ntrace = tp.trace(result, row=0)\n\n# In debug mode, you can see parent rows\nif trace.merge_parents:\n    print(f\"Left parent: {trace.merge_parents.left}\")\n    print(f\"Right parent: {trace.merge_parents.right}\")\n</code></pre>"},{"location":"guide/row-tracing/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Row tracing in CI mode is limited (no individual row IDs)</li> <li>For large DataFrames, use <code>where=</code> with indexed columns for faster lookups</li> <li>Tracing many rows? Use <code>tp.debug.inspect()</code> for batch access</li> </ul>"},{"location":"guide/snapshots/","title":"Snapshots &amp; Diff","text":"<p>Compare DataFrame states at different points in your pipeline.</p>"},{"location":"guide/snapshots/#taking-snapshots","title":"Taking Snapshots","text":"<pre><code># Capture current state\nsnapshot = tp.snapshot(df)\n\nprint(f\"Rows: {snapshot.n_rows}\")\nprint(f\"Columns: {snapshot.columns}\")\n</code></pre>"},{"location":"guide/snapshots/#comparing-snapshots","title":"Comparing Snapshots","text":"<pre><code># Before transformation\nbefore = tp.snapshot(df)\n\n# Apply transformations\ndf = df.dropna()\ndf[\"price\"] = df[\"price\"] * 1.1\n\n# After transformation\nafter = tp.snapshot(df)\n\n# Compare\ndiff = tp.diff(before, after)\nprint(diff)\n</code></pre> <p>Output:</p> <pre><code>Snapshot Diff:\n  Rows: 1000 \u2192 847 (-153)\n  Columns: ['id', 'price', 'qty'] \u2192 ['id', 'price', 'qty'] (unchanged)\n\n  Changes:\n    - 153 rows removed\n    - 847 cells modified in 'price'\n</code></pre>"},{"location":"guide/snapshots/#the-snapshot-object","title":"The Snapshot Object","text":"<pre><code>snapshot = tp.snapshot(df)\n\n# Access fields\nsnapshot.n_rows        # int: number of rows\nsnapshot.n_cols        # int: number of columns\nsnapshot.columns       # list[str]: column names\nsnapshot.dtypes        # dict: column dtypes\nsnapshot.row_ids       # set[int]: TracePipe row IDs (if available)\nsnapshot.timestamp     # datetime: when snapshot was taken\n\n# Data access (optional, if include_data=True)\nsnapshot.data          # DataFrame copy (if captured)\n</code></pre>"},{"location":"guide/snapshots/#the-diffresult-object","title":"The DiffResult Object","text":"<pre><code>diff = tp.diff(before, after)\n\n# Access fields\ndiff.rows_added        # int: new rows\ndiff.rows_removed      # int: removed rows\ndiff.rows_unchanged    # int: unchanged rows\ndiff.cells_changed     # int: modified cells\n\n# Column changes\ndiff.columns_added     # list[str]: new columns\ndiff.columns_removed   # list[str]: removed columns\n\n# Detailed changes (if both snapshots have data)\ndiff.changed_rows      # set[int]: IDs of changed rows\ndiff.changes_by_column # dict: {col: count}\n</code></pre>"},{"location":"guide/snapshots/#options","title":"Options","text":""},{"location":"guide/snapshots/#include-data","title":"Include Data","text":"<p>By default, snapshots don't store the actual DataFrame data (for memory efficiency). To include it:</p> <pre><code>snapshot = tp.snapshot(df, include_data=True)\n\n# Now you can access the data\nprint(snapshot.data.head())\n</code></pre>"},{"location":"guide/snapshots/#save-to-disk","title":"Save to Disk","text":"<pre><code># Save snapshot\ntp.snapshot(df, path=\"checkpoint_1.npz\")\n\n# Load later\n# (Requires include_data=True when saving)\n</code></pre>"},{"location":"guide/snapshots/#use-cases","title":"Use Cases","text":""},{"location":"guide/snapshots/#debugging-transformations","title":"Debugging Transformations","text":"<pre><code>def investigate_drop():\n    before = tp.snapshot(df)\n    result = df.dropna()\n    after = tp.snapshot(result)\n\n    diff = tp.diff(before, after)\n    print(f\"dropna removed {diff.rows_removed} rows\")\n    return result\n</code></pre>"},{"location":"guide/snapshots/#ab-comparison","title":"A/B Comparison","text":"<pre><code># Original pipeline\ntp.enable()\ndf_a = process_pipeline_v1(data)\nsnapshot_a = tp.snapshot(df_a)\n\n# Modified pipeline  \ntp.reset()\ndf_b = process_pipeline_v2(data)\nsnapshot_b = tp.snapshot(df_b)\n\n# Compare\ndiff = tp.diff(snapshot_a, snapshot_b)\nprint(f\"V2 has {diff.rows_added - diff.rows_removed} net rows\")\n</code></pre>"},{"location":"guide/snapshots/#checkpoint-validation","title":"Checkpoint Validation","text":"<pre><code>checkpoints = []\n\ndf = pd.read_csv(\"data.csv\")\ncheckpoints.append((\"load\", tp.snapshot(df)))\n\ndf = df.dropna()\ncheckpoints.append((\"clean\", tp.snapshot(df)))\n\ndf = df.merge(lookup, on=\"id\")\ncheckpoints.append((\"enrich\", tp.snapshot(df)))\n\n# Review pipeline stages\nfor i in range(1, len(checkpoints)):\n    name, snap = checkpoints[i]\n    prev_name, prev_snap = checkpoints[i-1]\n    diff = tp.diff(prev_snap, snap)\n    print(f\"{prev_name} \u2192 {name}: {diff.rows_removed} dropped, {diff.rows_added} added\")\n</code></pre>"},{"location":"guide/snapshots/#performance-notes","title":"Performance Notes","text":"<ul> <li>Snapshots without data are very lightweight (just metadata)</li> <li>Snapshots with data create a full DataFrame copy</li> <li>For large DataFrames, consider snapshotting only row IDs (default behavior)</li> </ul>"}]}
